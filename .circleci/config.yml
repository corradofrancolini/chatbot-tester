# Chatbot Tester - CircleCI Configuration
# Esegue test automatici su chatbot web nel cloud

version: 2.1

# Orbs per funzionalità comuni
orbs:
  python: circleci/python@2.1.1
  browser-tools: circleci/browser-tools@1.4.8

# Parametri per trigger manuale via API
parameters:
  project:
    type: string
    default: "my-chatbot"
  mode:
    type: enum
    enum: ["auto", "assisted", "train"]
    default: "auto"
  tests:
    type: enum
    enum: ["all", "pending", "failed"]
    default: "pending"
  new_run:
    type: boolean
    default: false
  test_limit:
    type: integer
    default: 0  # 0 = no limit, otherwise limit to N tests
  test_ids:
    type: string
    default: ""  # Comma-separated test IDs (e.g., "TEST_006,TEST_007,TEST_008")
  single_turn:
    type: boolean
    default: false  # Only initial question, no follow-ups
  tests_file:
    type: string
    default: "tests.json"  # Test set file to use (e.g., "tests_paraphrase.json")
  prompt_version:
    type: string
    default: ""  # Prompt version to use (e.g., "v12")
  repeat:
    type: integer
    default: 1  # Number of parallel runs (1-5) for variance analysis
  parallel_browsers:
    type: integer
    default: 0  # 0 = sequential, 1-5 = parallel with N browsers (Python-level)
  native_parallelism:
    type: integer
    default: 0  # 0 = disabled, 2-5 = split tests across N CircleCI containers
  manual_trigger:
    type: boolean
    default: false  # Only runs when triggered via API with manual_trigger=true
  sheet_prefix:
    type: string
    default: "Run"  # Prefix for sheet name (e.g., "GGP" for "GGP 001")
  skip_screenshots:
    type: boolean
    default: false  # Skip screenshot capture
  testset_standard:
    type: boolean
    default: true  # Run standard test set (tests.json)
  testset_paraphrase:
    type: boolean
    default: false  # Run paraphrase test set (tests_paraphrase.json)
  testset_ggp:
    type: boolean
    default: false  # Run GGP test set (tests_ggp.json)
  multi_testset:
    type: boolean
    default: false  # Run all 3 test sets in parallel (overrides individual flags)

# Esecutori
executors:
  python-browser:
    docker:
      - image: cimg/python:3.11-browsers
    resource_class: medium
    environment:
      PLAYWRIGHT_BROWSERS_PATH: /home/circleci/.cache/ms-playwright
      TZ: Europe/Rome

# Comandi riutilizzabili
commands:
  setup-environment:
    description: "Setup Python environment and dependencies"
    steps:
      - checkout
      # Cache pip dependencies
      - restore_cache:
          keys:
            - pip-v1-{{ checksum "requirements.txt" }}
            - pip-v1-
      - python/install-packages:
          pkg-manager: pip
          pip-dependency-file: requirements.txt
      - save_cache:
          key: pip-v1-{{ checksum "requirements.txt" }}
          paths:
            - ~/.cache/pip
            - ~/.local/lib/python3.11/site-packages
      # Cache Playwright browsers
      - restore_cache:
          keys:
            - playwright-v1-chromium
      - run:
          name: Install Playwright
          command: |
            pip install playwright
            playwright install chromium
            playwright install-deps chromium
      - save_cache:
          key: playwright-v1-chromium
          paths:
            - /home/circleci/.cache/ms-playwright

  setup-credentials:
    description: "Setup Google and LangSmith credentials"
    steps:
      - run:
          name: Setup credentials
          command: |
            mkdir -p config

            # LangSmith API Key
            if [ -n "$LANGSMITH_API_KEY" ]; then
              echo "LANGSMITH_API_KEY=$LANGSMITH_API_KEY" > config/.env
              echo "✓ LangSmith configured"
            fi

            # Google OAuth credentials
            if [ -n "$GOOGLE_OAUTH_CREDENTIALS" ]; then
              echo "$GOOGLE_OAUTH_CREDENTIALS" > config/oauth_credentials.json
              echo "✓ Google OAuth credentials configured"
            fi

            # Google Service Account (priorità - nessuna scadenza!)
            if [ -n "$GOOGLE_SERVICE_ACCOUNT" ]; then
              echo "$GOOGLE_SERVICE_ACCOUNT" > config/service_account.json
              echo "✓ Google Service Account configured (no expiry)"
            fi

            # Google OAuth Token (per upload screenshot su Drive personale)
            if [ -n "$GOOGLE_TOKEN_JSON" ]; then
              echo "$GOOGLE_TOKEN_JSON" > config/token.json
              echo "✓ Google OAuth token configured"
            fi

  setup-browser-auth:
    description: "Setup browser authentication state"
    parameters:
      project:
        type: string
    steps:
      - run:
          name: Setup browser auth state
          command: |
            if [ -n "$BROWSER_AUTH_STATE" ]; then
              mkdir -p projects/<< parameters.project >>/browser-data
              echo "$BROWSER_AUTH_STATE" | base64 -d > projects/<< parameters.project >>/browser-data/state.json
              echo "✓ Browser auth state loaded for << parameters.project >>"
            else
              echo "⚠ No browser auth state configured"
            fi

# Jobs
jobs:
  run-tests:
    executor: python-browser
    parameters:
      project:
        type: string
        default: "my-chatbot"
      mode:
        type: string
        default: "auto"
      tests:
        type: string
        default: "pending"
      new_run:
        type: boolean
        default: false
      test_limit:
        type: integer
        default: 0
      test_ids:
        type: string
        default: ""
      single_turn:
        type: boolean
        default: false
      tests_file:
        type: string
        default: "tests.json"
      prompt_version:
        type: string
        default: ""
      parallel_browsers:
        type: integer
        default: 0  # 0 = sequential, 1-5 = parallel with N browsers
      run_label:
        type: string
        default: ""  # Optional label for repeated runs (e.g., "run-1")
      sheet_prefix:
        type: string
        default: "Run"
      skip_screenshots:
        type: boolean
        default: false
    steps:
      - setup-environment
      - setup-credentials
      - setup-browser-auth:
          project: << parameters.project >>

      - run:
          name: Verify project exists
          command: |
            if [ ! -d "projects/<< parameters.project >>" ]; then
              echo "ERROR: Project << parameters.project >> not found"
              exit 1
            fi
            echo "✓ Project << parameters.project >> found"
            ls -la projects/<< parameters.project >>/

      - run:
          name: Health check
          command: |
            python run.py --health-check -p << parameters.project >> || true

      - run:
          name: Execute tests
          no_output_timeout: 30m
          command: |
            NEW_RUN_FLAG=""
            if [ "<< parameters.new_run >>" == "true" ]; then
              NEW_RUN_FLAG="--new-run"
            fi

            TEST_LIMIT_FLAG=""
            if [ "<< parameters.test_limit >>" != "0" ]; then
              TEST_LIMIT_FLAG="--test-limit << parameters.test_limit >>"
            fi

            TEST_IDS_FLAG=""
            if [ -n "<< parameters.test_ids >>" ]; then
              TEST_IDS_FLAG="--test-ids << parameters.test_ids >>"
            fi

            SINGLE_TURN_FLAG=""
            if [ "<< parameters.single_turn >>" == "true" ]; then
              SINGLE_TURN_FLAG="--single-turn"
            fi

            TESTS_FILE_FLAG=""
            if [ "<< parameters.tests_file >>" != "tests.json" ]; then
              TESTS_FILE_FLAG="--tests-file << parameters.tests_file >>"
            fi

            PARALLEL_FLAG=""
            if [ "<< parameters.parallel_browsers >>" != "0" ]; then
              PARALLEL_FLAG="--parallel --workers << parameters.parallel_browsers >>"
            fi

            PROMPT_VERSION_FLAG=""
            if [ -n "<< parameters.prompt_version >>" ]; then
              PROMPT_VERSION_FLAG="--prompt-version << parameters.prompt_version >>"
            fi

            SHEET_PREFIX_FLAG=""
            if [ "<< parameters.sheet_prefix >>" != "Run" ]; then
              SHEET_PREFIX_FLAG="--sheet-prefix << parameters.sheet_prefix >>"
            fi

            SKIP_SCREENSHOTS_FLAG=""
            if [ "<< parameters.skip_screenshots >>" == "true" ]; then
              SKIP_SCREENSHOTS_FLAG="--skip-screenshots"
            fi

            python run.py \
              -p << parameters.project >> \
              -m << parameters.mode >> \
              --tests << parameters.tests >> \
              --no-interactive \
              --headless \
              --skip-health-check \
              $NEW_RUN_FLAG \
              $TEST_LIMIT_FLAG \
              $TEST_IDS_FLAG \
              $SINGLE_TURN_FLAG \
              $TESTS_FILE_FLAG \
              $PARALLEL_FLAG \
              $PROMPT_VERSION_FLAG \
              $SHEET_PREFIX_FLAG \
              $SKIP_SCREENSHOTS_FLAG

      - store_artifacts:
          path: reports
          destination: test-reports

      - run:
          name: Generate summary
          when: always
          command: |
            echo "=== Test Summary ==="
            echo "Project: << parameters.project >>"
            echo "Mode: << parameters.mode >>"
            echo "Tests: << parameters.tests >>"
            if [ -d "reports" ]; then
              echo "Reports generated:"
              find reports -name "*.html" -o -name "*.json" | head -20
            fi

  # Job con parallelismo nativo CircleCI per splitting test
  run-tests-native-parallel:
    executor: python-browser
    parallelism: << parameters.parallelism >>
    parameters:
      project:
        type: string
      mode:
        type: string
        default: "auto"
      tests:
        type: string
        default: "pending"
      new_run:
        type: boolean
        default: false
      test_ids:
        type: string
        default: ""
      single_turn:
        type: boolean
        default: false
      tests_file:
        type: string
        default: "tests.json"
      prompt_version:
        type: string
        default: ""
      parallelism:
        type: integer
        default: 3
      sheet_prefix:
        type: string
        default: "Run"
      skip_screenshots:
        type: boolean
        default: false
    steps:
      - setup-environment
      - setup-credentials
      - setup-browser-auth:
          project: << parameters.project >>

      - run:
          name: Split tests across containers
          command: |
            # Se test_ids è specificato, splitta quelli
            if [ -n "<< parameters.test_ids >>" ]; then
              # Converti comma-separated in newline-separated
              echo "<< parameters.test_ids >>" | tr ',' '\n' > /tmp/all_tests.txt

              # Usa circleci tests split per distribuire
              cat /tmp/all_tests.txt | circleci tests split > /tmp/my_tests.txt

              # Converti back to comma-separated
              MY_TEST_IDS=$(cat /tmp/my_tests.txt | tr '\n' ',' | sed 's/,$//')
              echo "export MY_TEST_IDS=$MY_TEST_IDS" >> $BASH_ENV

              echo "Container $CIRCLE_NODE_INDEX of $CIRCLE_NODE_TOTAL"
              echo "My tests: $MY_TEST_IDS"
            else
              echo "No test_ids specified, will run << parameters.tests >> tests"
              echo "export MY_TEST_IDS=" >> $BASH_ENV
            fi

      - run:
          name: Execute tests
          no_output_timeout: 30m
          command: |
            # Solo il primo container crea new_run per evitare RUN multiple
            NEW_RUN_FLAG=""
            if [ "<< parameters.new_run >>" == "true" ] && [ "$CIRCLE_NODE_INDEX" == "0" ]; then
              NEW_RUN_FLAG="--new-run"
              echo "Container 0: Creating new RUN"
            elif [ "<< parameters.new_run >>" == "true" ]; then
              # Aspetta che container 0 crei la RUN prima di procedere
              echo "Container $CIRCLE_NODE_INDEX: Waiting 30s for container 0 to create RUN..."
              sleep 30
              echo "Container $CIRCLE_NODE_INDEX: Proceeding to write to latest RUN"
            fi

            SINGLE_TURN_FLAG=""
            if [ "<< parameters.single_turn >>" == "true" ]; then
              SINGLE_TURN_FLAG="--single-turn"
            fi

            TEST_IDS_FLAG=""
            if [ -n "$MY_TEST_IDS" ]; then
              TEST_IDS_FLAG="--test-ids $MY_TEST_IDS"
            fi

            TESTS_FILE_FLAG=""
            if [ "<< parameters.tests_file >>" != "tests.json" ]; then
              TESTS_FILE_FLAG="--tests-file << parameters.tests_file >>"
            fi

            PROMPT_VERSION_FLAG=""
            if [ -n "<< parameters.prompt_version >>" ]; then
              PROMPT_VERSION_FLAG="--prompt-version << parameters.prompt_version >>"
            fi

            SHEET_PREFIX_FLAG=""
            if [ "<< parameters.sheet_prefix >>" != "Run" ]; then
              SHEET_PREFIX_FLAG="--sheet-prefix << parameters.sheet_prefix >>"
            fi

            SKIP_SCREENSHOTS_FLAG=""
            if [ "<< parameters.skip_screenshots >>" == "true" ]; then
              SKIP_SCREENSHOTS_FLAG="--skip-screenshots"
            fi

            python run.py \
              -p << parameters.project >> \
              -m << parameters.mode >> \
              --tests << parameters.tests >> \
              --no-interactive \
              --headless \
              --skip-health-check \
              $NEW_RUN_FLAG \
              $SINGLE_TURN_FLAG \
              $TEST_IDS_FLAG \
              $TESTS_FILE_FLAG \
              $PROMPT_VERSION_FLAG \
              $SHEET_PREFIX_FLAG \
              $SKIP_SCREENSHOTS_FLAG

      - store_artifacts:
          path: reports
          destination: test-reports-node-$CIRCLE_NODE_INDEX

# Workflows
workflows:
  version: 2

  # Trigger manuale via API (singola run, senza native parallelism)
  manual-test:
    when:
      and:
        - << pipeline.parameters.manual_trigger >>
        - equal: [1, << pipeline.parameters.repeat >>]
        - equal: [0, << pipeline.parameters.native_parallelism >>]
        - not: << pipeline.parameters.multi_testset >>
    jobs:
      - run-tests:
          name: "test-<< pipeline.parameters.project >>-<< pipeline.parameters.mode >>"
          project: << pipeline.parameters.project >>
          mode: << pipeline.parameters.mode >>
          tests: << pipeline.parameters.tests >>
          new_run: << pipeline.parameters.new_run >>
          test_limit: << pipeline.parameters.test_limit >>
          test_ids: << pipeline.parameters.test_ids >>
          single_turn: << pipeline.parameters.single_turn >>
          tests_file: << pipeline.parameters.tests_file >>
          prompt_version: << pipeline.parameters.prompt_version >>
          parallel_browsers: << pipeline.parameters.parallel_browsers >>
          sheet_prefix: << pipeline.parameters.sheet_prefix >>
          skip_screenshots: << pipeline.parameters.skip_screenshots >>

  # Trigger manuale con native parallelism (split tests across N containers)
  native-parallel-test:
    when:
      and:
        - << pipeline.parameters.manual_trigger >>
        - not:
            equal: [0, << pipeline.parameters.native_parallelism >>]
    jobs:
      - run-tests-native-parallel:
          name: "<< pipeline.parameters.project >>-<< pipeline.parameters.mode >>-parallel"
          project: << pipeline.parameters.project >>
          mode: << pipeline.parameters.mode >>
          tests: << pipeline.parameters.tests >>
          new_run: << pipeline.parameters.new_run >>
          test_ids: << pipeline.parameters.test_ids >>
          single_turn: << pipeline.parameters.single_turn >>
          tests_file: << pipeline.parameters.tests_file >>
          prompt_version: << pipeline.parameters.prompt_version >>
          parallelism: << pipeline.parameters.native_parallelism >>
          sheet_prefix: << pipeline.parameters.sheet_prefix >>
          skip_screenshots: << pipeline.parameters.skip_screenshots >>

  # Trigger manuale con repeat (multiple run parallele per analisi varianza)
  repeated-test:
    when:
      and:
        - << pipeline.parameters.manual_trigger >>
        - not:
            equal: [1, << pipeline.parameters.repeat >>]
    jobs:
      - run-tests:
          name: "run-1-<< pipeline.parameters.project >>"
          project: << pipeline.parameters.project >>
          mode: << pipeline.parameters.mode >>
          tests: << pipeline.parameters.tests >>
          new_run: true
          test_limit: << pipeline.parameters.test_limit >>
          test_ids: << pipeline.parameters.test_ids >>
          single_turn: << pipeline.parameters.single_turn >>
          tests_file: << pipeline.parameters.tests_file >>
          prompt_version: << pipeline.parameters.prompt_version >>
          parallel_browsers: << pipeline.parameters.parallel_browsers >>
          run_label: "run-1"
      - run-tests:
          name: "run-2-<< pipeline.parameters.project >>"
          project: << pipeline.parameters.project >>
          mode: << pipeline.parameters.mode >>
          tests: << pipeline.parameters.tests >>
          new_run: true
          test_limit: << pipeline.parameters.test_limit >>
          test_ids: << pipeline.parameters.test_ids >>
          single_turn: << pipeline.parameters.single_turn >>
          tests_file: << pipeline.parameters.tests_file >>
          prompt_version: << pipeline.parameters.prompt_version >>
          parallel_browsers: << pipeline.parameters.parallel_browsers >>
          run_label: "run-2"
      - run-tests:
          name: "run-3-<< pipeline.parameters.project >>"
          project: << pipeline.parameters.project >>
          mode: << pipeline.parameters.mode >>
          tests: << pipeline.parameters.tests >>
          new_run: true
          test_limit: << pipeline.parameters.test_limit >>
          test_ids: << pipeline.parameters.test_ids >>
          single_turn: << pipeline.parameters.single_turn >>
          tests_file: << pipeline.parameters.tests_file >>
          prompt_version: << pipeline.parameters.prompt_version >>
          parallel_browsers: << pipeline.parameters.parallel_browsers >>
          run_label: "run-3"
      - run-tests:
          name: "run-4-<< pipeline.parameters.project >>"
          project: << pipeline.parameters.project >>
          mode: << pipeline.parameters.mode >>
          tests: << pipeline.parameters.tests >>
          new_run: true
          test_limit: << pipeline.parameters.test_limit >>
          test_ids: << pipeline.parameters.test_ids >>
          single_turn: << pipeline.parameters.single_turn >>
          tests_file: << pipeline.parameters.tests_file >>
          prompt_version: << pipeline.parameters.prompt_version >>
          parallel_browsers: << pipeline.parameters.parallel_browsers >>
          run_label: "run-4"
      - run-tests:
          name: "run-5-<< pipeline.parameters.project >>"
          project: << pipeline.parameters.project >>
          mode: << pipeline.parameters.mode >>
          tests: << pipeline.parameters.tests >>
          new_run: true
          test_limit: << pipeline.parameters.test_limit >>
          test_ids: << pipeline.parameters.test_ids >>
          single_turn: << pipeline.parameters.single_turn >>
          tests_file: << pipeline.parameters.tests_file >>
          prompt_version: << pipeline.parameters.prompt_version >>
          parallel_browsers: << pipeline.parameters.parallel_browsers >>
          run_label: "run-5"

  # Multi test set - 3 test set in parallelo
  multi-testset-test:
    when:
      and:
        - << pipeline.parameters.manual_trigger >>
        - << pipeline.parameters.multi_testset >>
    jobs:
      - run-tests:
          name: "standard-<< pipeline.parameters.project >>"
          project: << pipeline.parameters.project >>
          mode: << pipeline.parameters.mode >>
          tests: << pipeline.parameters.tests >>
          new_run: << pipeline.parameters.new_run >>
          test_limit: << pipeline.parameters.test_limit >>
          single_turn: << pipeline.parameters.single_turn >>
          tests_file: "tests.json"
          prompt_version: << pipeline.parameters.prompt_version >>
          sheet_prefix: "Run"
          skip_screenshots: << pipeline.parameters.skip_screenshots >>
      - run-tests:
          name: "paraphrase-<< pipeline.parameters.project >>"
          project: << pipeline.parameters.project >>
          mode: << pipeline.parameters.mode >>
          tests: << pipeline.parameters.tests >>
          new_run: << pipeline.parameters.new_run >>
          test_limit: << pipeline.parameters.test_limit >>
          single_turn: << pipeline.parameters.single_turn >>
          tests_file: "tests_paraphrase.json"
          prompt_version: << pipeline.parameters.prompt_version >>
          sheet_prefix: "Paraphrase"
          skip_screenshots: << pipeline.parameters.skip_screenshots >>
      - run-tests:
          name: "ggp-<< pipeline.parameters.project >>"
          project: << pipeline.parameters.project >>
          mode: << pipeline.parameters.mode >>
          tests: << pipeline.parameters.tests >>
          new_run: << pipeline.parameters.new_run >>
          test_limit: << pipeline.parameters.test_limit >>
          single_turn: << pipeline.parameters.single_turn >>
          tests_file: "tests_ggp.json"
          prompt_version: << pipeline.parameters.prompt_version >>
          sheet_prefix: "GGP"
          skip_screenshots: << pipeline.parameters.skip_screenshots >>
